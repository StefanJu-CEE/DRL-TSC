[simulation] 
gui = False
total_episodes = 100
max_steps = 5400
n_cars_generated =2000
green_duration = 15
yellow_duration = 4

[model]
num_layers = 4
width_layers = 256
batch_size = 96
learning_rate = 0.0003
training_epochs = 150

[memory]
memory_size_min = 2000
memory_size_max = 50000

[agent]
num_states = 80
num_actions = 4
gamma = 0.95

[exploration]
epsilon_start = 1.0
epsilon_end = 0.05
decay_rate = 0.97
decay_type = steps  # steps or episodes

[congestion_penalty]
congestion_threshold = 25  
congestion_penalty = -10
waiting_time_penalty_scale = 0.0

[state_normalization]

state_normalization = z_score  

[training_stability]

target_value_clip_min = -1000  
target_value_clip_max = 1000 

[experience_retention]
experience_retention_ratio = 0.4  
max_buffer_usage = 0.4            
retention_strategy = "smart"      
high_value_ratio = 0.6            
diverse_ratio = 0.4               

[d3qn]
target_update_freq = 1000
per_alpha = 0.4
per_beta = 0.3
per_eps = 1e-6
per_beta_increment = 0.004

[dir]
models_path_name = models
sumocfg_file_name = sumo_config.sumocfg
construction_sumocfg_file_name = sumo_config_obs1.sumocfg

[reward]
flow_reward_weight = 0.15
# 新增全局效率参数 - 调整为主要权重
global_efficiency_weight = 0.8
efficiency_threshold = 5.0
efficiency_bonus = 5.0
efficiency_penalty_multiplier = 2.0

[action_constraints]
max_green = 50
imbalance_ratio = 1.8
max_wait_threshold = 80.0

[PER]
use_per = False
uniform_mix_ratio = 0.2
priority_clip_max = 10.0

